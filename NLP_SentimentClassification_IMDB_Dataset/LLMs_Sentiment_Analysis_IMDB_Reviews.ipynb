{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fec1b2-27ea-4f55-8ac3-5f85054a80e6",
   "metadata": {},
   "source": [
    "# LLMs Assignment- IMDB Sentiment Analysis Dataset- Mohamed Helayhel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "223d7090-5d4d-41c8-8836-6b3a9b7ebd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import imdb \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec886312-b117-4d82-9409-c36f9f522611",
   "metadata": {},
   "source": [
    "## Load Dataset- IMDB\n",
    "The IMDB dataset is a collection of 50,000 movie reviews from the Internet Movie Database (IMDB) that have been preprocessed and labeled by sentiment (positive/negative). The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers).\n",
    "\n",
    "**Desired Output:** The objective is to analyze the words in a movie review sequentially and predict whether the review is positive or negative using LLM pre-trained models (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a24afe8-e1c7-4a2d-8c35-b54744e602fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28196515-26ac-4473-9b8c-d336f8964b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06e4e0ab-5000-4fd5-9407-a03b1144ff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9b00524-183e-42f4-a868-98f4c46de632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "595340b6-a91b-4209-9b80-e9787d50f42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f979e11-71ff-4666-9c63-b2de8bdb81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd86e7ee-bada-43a4-8913-7bbbba92eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b522219c-70f1-4f87-9e4c-f9c44740095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763aab85-6efd-4859-9a57-6d68d4a17bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ea3331a52e4026953a756d966e3e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie. It was fantastic!\n",
      "Sentiment: {'label': '5 stars', 'score': 0.9433344006538391}\n",
      "\n",
      "Text: The movie was boring and terrible.\n",
      "Sentiment: {'label': '1 star', 'score': 0.6183295249938965}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a sentiment-analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Analyze a few examples\n",
    "examples = [\"I love this movie. It was fantastic!\", \"The movie was boring and terrible.\"]\n",
    "results = sentiment_pipeline(examples)\n",
    "\n",
    "# Display results\n",
    "for example, result in zip(examples, results):\n",
    "    print(f\"Text: {example}\\nSentiment: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eaf4db-4f04-43f3-823b-a315ca73190f",
   "metadata": {},
   "source": [
    "### Initial Exploration of BERT LLM Modeling\n",
    "The code above uses the pre-trained sentiment analysis model pipeline with bert as a base from the transformers library on Hugging Face,\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for understanding the context of words in a sentence by considering both their left and right contexts simultaneously. It uses Masked Language Modeling (MLM), i.e. it randomly masks some words in a sentence and trains the model to predict them based on context. It also uses Next Sentence Prediction (NSP), i.e. predicts whether one sentence logically follows another, helping in understanding sentence relationships. It is an Encoder only model, which relies on the attention mechanism to weigh the importance of each word in a sequence relative to others. Above you can see an example of how BERT works to classify a sample review. It assigns a score, where a higher score corresponds to a better rating, and it correctly labels each review with a 5 star for the positive review and 1 start for the negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdaead9c-28c0-4b01-94ed-79da4e0b60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e3cf7-1b2b-49cc-af48-ced1a403217d",
   "metadata": {},
   "source": [
    "### Further Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e25d5b-4ed8-4520-9ac6-77a904591404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Check the structure of the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Convert to a DataFrame (for easier handling)\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Display a sample\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed420e4-fbf4-4d0d-a1ed-ae9eadec760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  3185,  2001,  7078, 10392,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Load a tokenizer (BERT tokenizer in this example)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize a single example\n",
    "sample_text = \"This movie was absolutely fantastic!\"\n",
    "tokens = tokenizer(sample_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbd1b5-dd87-48db-9343-ee2fe12a8e96",
   "metadata": {},
   "source": [
    "The code above shows how the tokenizer works on a single example. The BERT tokenizer uses WordPiece tokenization, which breaks words into subwords or characters if they are not in the vocabulary. It converts text to a sequence of token IDs while adding special tokens like [CLS] (classification) and [SEP] (separator). This allows BERT to handle out-of-vocabulary words and capture the contextual meaning of text efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cec1d00-b960-4def-8fb1-02c78a0093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400e8ecb-ba64-42e5-9de9-4ffefb3ec884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the text into input IDs\n",
    "def encode_example(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Apply encoding to the dataset\n",
    "train_data = train_data.map(encode_example, batched=True)\n",
    "test_data = test_data.map(encode_example, batched=True)\n",
    "\n",
    "# Format dataset for PyTorch\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d4f234-caf8-405e-852d-22bc095a1e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cce25a-f4b5-4185-9035-dc1e0294d832",
   "metadata": {},
   "source": [
    "### Model Design Summary\n",
    "In the code above, the berttokenizer was imported and applied on the entire dataset. Pytorch was used to set the format of the data in such a way that takes input_ids, attention_mask, and label. Then, the model BertForSequenceClassification was imported from hugging face's transformers architecture, and the 'bert-base-uncased' was imported. The num_labels was set to 2 since it was done with goal of classification of reviews to only two categories: positive or negative.\n",
    "\n",
    "In the code below, we design the specific model. Key elements are:\n",
    "1. AdamW Optimizer Initialization: a variation of the Adam optimizer that decouples weight decay from the learning rate, which is particularly effective for transformer models like BERT. The lr=2e-5 is a typical learning rate for fine-tuning pre-trained models.\n",
    "2. output_dir: Specifies where the model checkpoints and logs will be saved, which is 'results' in this case.\n",
    "3. evaluation_strategy=\"epoch\": Triggers evaluation after every epoch, which used for monitoring validation performance.\n",
    "4. learning_rate: Although specified here, it is overridden by the explicit optimizer provided later.\n",
    "5. weight_decay=0.01: Encourages smaller weights, which helps reduce overfitting.\n",
    "6. per_device_train_batch_size and per_device_eval_batch_size: These values are typical but can be adjusted based on GPU memory capacity, as well as model performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd3599c-5dcf-4550-ae85-475bf8b5c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Initialize AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    optimizers=(optimizer, None),  # Use AdamW optimizer without learning rate scheduler\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab2e4f5-9e84-4a69-9e8a-c6321f8b6ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 31:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>0.285079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.321723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.465134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4689, training_loss=0.21579934549524882, metrics={'train_runtime': 1900.9856, 'train_samples_per_second': 39.453, 'train_steps_per_second': 2.467, 'total_flos': 4933332288000000.0, 'train_loss': 0.21579934549524882, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e626-0c02-4898-9763-04965941bf96",
   "metadata": {},
   "source": [
    "### Comment on Model Above:\n",
    "The model returned the lowest validation loss for the first epoch, which shows that it learned as much as it could from the first round. Future rounds show the model lowering its training loss but increasing its validation loss, a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bf21b86-119d-46df-b5fd-b369779c27a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.46513429284095764\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print results (accuracy, loss, etc.)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_accuracy'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print results (accuracy, loss, etc.)\n",
    "print(f\"Test Loss: {results['eval_loss']}\")\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cedde-a2fd-4449-9e9f-5dc5806ec04b",
   "metadata": {},
   "source": [
    "## Function to generate more comprehensive accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f7761b03-095d-478a-afef-812e64244b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "   # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfff16c-4ec2-4b75-b17a-718e56484348",
   "metadata": {},
   "source": [
    "### Same Model parameters as above: 1 Epoch instead of 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19bc1219-7d3b-45b3-99c3-4cf3331e25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Initialize AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    optimizers=(optimizer, None), # Use AdamW optimizer without learning rate scheduler\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8afd66a-9047-4860-9dd0-df280496fbc6",
   "metadata": {},
   "source": [
    "### Comment on Model Above:\n",
    "Given that the best learning happens in epoch 1, the next model only used 1 epoch and the training time was reduced from 31 mins to 13 minutes, a drastic improvement in terms of computational intensity. However, we see that the validation loss in this 1 epoch, despite keeping all other parameters similar, is higher than all 3 above. This is typical given the non-deterministic nature of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b01da4b6-dc9a-4662-8b91-aabed4d8b499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 13:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.565154</td>\n",
       "      <td>0.886120</td>\n",
       "      <td>0.886187</td>\n",
       "      <td>0.886120</td>\n",
       "      <td>0.886115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5651538968086243, 'eval_accuracy': 0.88612, 'eval_precision': 0.886187289273283, 'eval_recall': 0.88612, 'eval_f1': 0.8861150391711062, 'eval_runtime': 151.9552, 'eval_samples_per_second': 164.522, 'eval_steps_per_second': 2.573, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3ecefc2-442e-4840-af47-11ac00334db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 12:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.519798</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.885724</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.885720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5197984576225281, 'eval_accuracy': 0.88572, 'eval_precision': 0.885724149774693, 'eval_recall': 0.88572, 'eval_f1': 0.8857196926316854, 'eval_runtime': 152.7045, 'eval_samples_per_second': 163.715, 'eval_steps_per_second': 2.561, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6aed4-afb2-456c-899f-2a436cdff76c",
   "metadata": {},
   "source": [
    "### Comment on code above\n",
    "Retraining the same model gave us slightly different accuracy results, again showcasing the non-deterministic nature of LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f227cf-8240-4220-80c7-ae7ad47c19ca",
   "metadata": {},
   "source": [
    "### Using different Optimizer: SGD instead of AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cb2caf5-be84-4bb5-8398-73e8a0e7aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "# Initialize AdamW optimizer\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f0229e0-c352-475f-9a8d-42014c5b124b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 32:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.571688</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.878998</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.876448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.628698</td>\n",
       "      <td>0.883680</td>\n",
       "      <td>0.884325</td>\n",
       "      <td>0.883680</td>\n",
       "      <td>0.883631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.656673</td>\n",
       "      <td>0.887560</td>\n",
       "      <td>0.887692</td>\n",
       "      <td>0.887560</td>\n",
       "      <td>0.887550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 03:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6566731929779053, 'eval_accuracy': 0.88756, 'eval_precision': 0.8876924009869381, 'eval_recall': 0.88756, 'eval_f1': 0.8875503993229732, 'eval_runtime': 207.9523, 'eval_samples_per_second': 120.22, 'eval_steps_per_second': 1.88, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c88bd-6898-4854-8b4b-511608842e4b",
   "metadata": {},
   "source": [
    "### Comment on code above:\n",
    "Using SGD instead of AdamW presented similar results. The key reason for this is pre-trained Model Stability. Fine-tuning pre-trained models like BERT often leads to good results regardless of the optimizer because the model already has well-initialized parameters. This can reduce the observable differences between optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ca9a1a3-23df-4077-8f4a-b0c2f470db90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to evaluate confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate predictions for confusion matrix\n",
    "predictions = trainer.predict(test_data)\n",
    "y_true = test_data['label']\n",
    "y_pred = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b361f-2660-4711-ae57-50f87413a498",
   "metadata": {},
   "source": [
    "### Evaluation of Model+ future work\n",
    "The confusion Matrix shows a fairly balanced case of both reading false positives and false negatives. This is unsurprising given BERT model's stability and its ability to understand context, so we shouldn't expect results to be skewed in either direction. Overall, the results are good, since the model, even with one epoch and minimal optimization was able to achieve an accuracy of almost 90% on a large test dataset of 25k samples, so the results are satisfactory given the computational limitations we faced. \n",
    "\n",
    "However, to further improve accuracy, we can consider the following steps:\n",
    "1. Hyperparameter Tuning: Optimize learning rates, batch sizes, and number of epochs.\n",
    "2. Learning Rate Scheduler: Use a scheduler (e.g., linear_schedule_with_warmup) for smoother convergence.\n",
    "3. Data Augmentation: Add diversity through synonym replacement, back-translation, or random insertions.\n",
    "4. Larger Dataset: Combine with similar datasets or perform unsupervised pretraining on domain-specific text.\n",
    "5. Alternative Models: Experiment with models like RoBERTa or DistilBERT for potential accuracy gains.\n",
    "7. Regularization: Increase dropout or adjust weight decay to prevent overfitting.\n",
    "8. Metrics and Evaluation: Use metrics like F1-Score and ROC-AUC for deeper insights.\n",
    "9. Ensemble Models: Combine predictions from multiple models for improved robustness.\n",
    "10. Hardware Optimization: Enable mixed precision training or leverage multiple GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
